# YouTube Shorts ë°ì´í„° ìˆ˜ì§‘ ì™„ì „ ê°€ì´ë“œ

Created: 2024ë…„ 12ì›”
Tags: YouTube Shorts, Data Collection, API, Web Scraping, Content Analysis
Author: AI Assistant

---

## ğŸ“‹ **ëª©ì°¨**

1. [ê°œìš” ë° ìˆ˜ì§‘ ê°€ëŠ¥í•œ ë°ì´í„°](#ê°œìš”-ë°-ìˆ˜ì§‘-ê°€ëŠ¥í•œ-ë°ì´í„°)
2. [ë²•ì  ê³ ë ¤ì‚¬í•­](#ë²•ì -ê³ ë ¤ì‚¬í•­)
3. [ë°©ë²•ë³„ ìƒì„¸ ê°€ì´ë“œ](#ë°©ë²•ë³„-ìƒì„¸-ê°€ì´ë“œ)
4. [í†µí•© ì†”ë£¨ì…˜](#í†µí•©-ì†”ë£¨ì…˜)
5. [ì‹¤ì œ êµ¬í˜„ ì˜ˆì œ](#ì‹¤ì œ-êµ¬í˜„-ì˜ˆì œ)
6. [ë°ì´í„° í™œìš© ì „ëµ](#ë°ì´í„°-í™œìš©-ì „ëµ)

---

## ğŸ¯ **ê°œìš” ë° ìˆ˜ì§‘ ê°€ëŠ¥í•œ ë°ì´í„°**

### **ìˆ˜ì§‘ ê°€ëŠ¥í•œ ì£¼ìš” ë°ì´í„°**

```mermaid
mindmap
  root)YouTube Shorts ë°ì´í„°(
    ê¸°ë³¸ ì •ë³´
      ì˜ìƒ ì œëª©
      ì˜ìƒ ì„¤ëª…
      ì—…ë¡œë“œ ë‚ ì§œ
      ì±„ë„ëª…
      ì±„ë„ êµ¬ë…ììˆ˜
    í†µê³„ ë°ì´í„°
      ì¡°íšŒìˆ˜
      ì¢‹ì•„ìš” ìˆ˜
      ì‹«ì–´ìš” ìˆ˜
      ëŒ“ê¸€ ìˆ˜
      ê³µìœ  ìˆ˜
    ëŒ“ê¸€ ë°ì´í„°
      ëŒ“ê¸€ ë‚´ìš©
      ëŒ“ê¸€ ì‘ì„±ì
      ëŒ“ê¸€ ì¢‹ì•„ìš” ìˆ˜
      ëŒ“ê¸€ ì‘ì„± ì‹œê°„
      ëŒ€ëŒ“ê¸€ ì •ë³´
    ë©”íƒ€ë°ì´í„°
      ì˜ìƒ ê¸¸ì´
      í•´ìƒë„
      ì¸ë„¤ì¼ URL
      íƒœê·¸ ì •ë³´
      ì¹´í…Œê³ ë¦¬
```

### **ë°ì´í„° ìˆ˜ì§‘ ë°©ë²• ë¹„êµ**

| ë°©ë²• | ë‚œì´ë„ | ì•ˆì „ì„± | ì™„ì„±ë„ | ë¹„ìš© | ì¶”ì²œë„ |
|------|--------|--------|--------|------|-------|
| **YouTube Data API v3** | ì¤‘ê°„ | â­â­â­â­â­ | â­â­â­â­ | ë¬´ë£Œ* | â­â­â­â­â­ |
| **yt-dlp** | ì‰¬ì›€ | â­â­â­â­ | â­â­â­â­ | ë¬´ë£Œ | â­â­â­â­ |
| **Selenium ìŠ¤í¬ë˜í•‘** | ì–´ë ¤ì›€ | â­â­ | â­â­â­ | ë¬´ë£Œ | â­â­ |
| **í†µí•© ì†”ë£¨ì…˜** | ì¤‘ê°„ | â­â­â­â­ | â­â­â­â­â­ | ë¬´ë£Œ* | â­â­â­â­â­ |

*ë¬´ë£Œ í• ë‹¹ëŸ‰ ì¡´ì¬

---

## âš–ï¸ **ë²•ì  ê³ ë ¤ì‚¬í•­**

### **âœ… ê¶Œì¥ë˜ëŠ” ë°©ë²•**

```mermaid
flowchart TD
    START[ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘] --> CHECK{ê³µì‹ API ì‚¬ìš© ê°€ëŠ¥?}
    CHECK -->|YES| API[YouTube Data API v3 ì‚¬ìš©]
    CHECK -->|NO| TERMS[ì´ìš©ì•½ê´€ ê²€í† ]
    
    API --> QUOTA[í• ë‹¹ëŸ‰ ê´€ë¦¬]
    QUOTA --> COLLECT[ë°ì´í„° ìˆ˜ì§‘]
    
    TERMS --> ROBOT[robots.txt í™•ì¸]
    ROBOT --> RATE[ìš”ì²­ ì†ë„ ì œí•œ]
    RATE --> RESPECTFUL[ì •ì¤‘í•œ ìŠ¤í¬ë˜í•‘]
    RESPECTFUL --> COLLECT
    
    COLLECT --> STORE[ì•ˆì „í•œ ì €ì¥]
    STORE --> USE[ì ë²•í•œ ì‚¬ìš©]
    
    style API fill:#e8f5e8
    style RESPECTFUL fill:#fff3e0
    style USE fill:#e1f5fe
```

### **ğŸš« í”¼í•´ì•¼ í•  í–‰ìœ„**

- âŒ ëŒ€ëŸ‰ì˜ ìë™í™”ëœ ìš”ì²­ (DDoSì™€ ìœ ì‚¬í•œ í–‰ìœ„)
- âŒ YouTubeì˜ ë³´ì•ˆ ì‹œìŠ¤í…œ ìš°íšŒ ì‹œë„
- âŒ ê°œì¸ì •ë³´ ë¬´ë‹¨ ìˆ˜ì§‘ ë° í™œìš©
- âŒ ìƒì—…ì  ëª©ì ì˜ ë¬´ë‹¨ ì¬ë°°í¬
- âŒ API í‚¤ ë¬´ë‹¨ ê³µìœ  ë˜ëŠ” ì•…ìš©

### **âœ… ëª¨ë²” ì‚¬ë¡€**

- âœ… ê³µì‹ API ìš°ì„  ì‚¬ìš©
- âœ… ìš”ì²­ ê°„ê²© ì ì ˆíˆ ì¡°ì ˆ (1ì´ˆ ì´ìƒ)
- âœ… ìˆ˜ì§‘ ëª©ì  ëª…í™•íˆ ì •ì˜
- âœ… ë°ì´í„° ë³´ì•ˆ ë° í”„ë¼ì´ë²„ì‹œ ë³´í˜¸
- âœ… ì´ìš©ì•½ê´€ ì •ê¸°ì  ê²€í† 

---

## ğŸ› ï¸ **ë°©ë²•ë³„ ìƒì„¸ ê°€ì´ë“œ**

### **ë°©ë²• 1: YouTube Data API v3 (ê¶Œì¥)**

#### **ğŸ“‹ ì‚¬ì „ ì¤€ë¹„**

1. Google Cloud Console í”„ë¡œì íŠ¸ ìƒì„±
2. YouTube Data API v3 í™œì„±í™”
3. API í‚¤ ìƒì„±
4. í• ë‹¹ëŸ‰ ì„¤ì • (ì¼ì¼ 10,000 ë‹¨ìœ„)

#### **ğŸ”§ ê¸°ë³¸ êµ¬í˜„**

```python
from googleapiclient.discovery import build
import requests
import json
from datetime import datetime
import re

class YouTubeDataCollector:
    def __init__(self, api_key):
        self.api_key = api_key
        self.youtube = build('youtube', 'v3', developerKey=api_key)
    
    def extract_video_id(self, url):
        """YouTube URLì—ì„œ video ID ì¶”ì¶œ"""
        patterns = [
            r'(?:youtube\.com/shorts/)([^&\n?#]+)',
            r'(?:youtube\.com/watch\?v=)([^&\n?#]+)',
            r'(?:youtu\.be/)([^&\n?#]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        return None
    
    def get_video_details(self, video_id):
        """ì˜ìƒ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘"""
        try:
            response = self.youtube.videos().list(
                part='snippet,statistics,contentDetails',
                id=video_id
            ).execute()
            
            if not response['items']:
                return None
            
            video = response['items'][0]
            snippet = video['snippet']
            statistics = video['statistics']
            
            return {
                'video_id': video_id,
                'title': snippet.get('title', ''),
                'description': snippet.get('description', ''),
                'channel_title': snippet.get('channelTitle', ''),
                'channel_id': snippet.get('channelId', ''),
                'published_at': snippet.get('publishedAt', ''),
                'tags': snippet.get('tags', []),
                'category_id': snippet.get('categoryId', ''),
                'view_count': int(statistics.get('viewCount', 0)),
                'like_count': int(statistics.get('likeCount', 0)),
                'comment_count': int(statistics.get('commentCount', 0)),
                'duration': video['contentDetails'].get('duration', ''),
            }
        except Exception as e:
            print(f"Error getting video details: {e}")
            return None
    
    def get_video_comments(self, video_id, max_results=100):
        """ëŒ“ê¸€ ìˆ˜ì§‘"""
        comments = []
        try:
            response = self.youtube.commentThreads().list(
                part='snippet,replies',
                videoId=video_id,
                maxResults=min(max_results, 100),
                order='relevance'
            ).execute()
            
            for item in response['items']:
                comment = item['snippet']['topLevelComment']['snippet']
                comment_data = {
                    'comment_id': item['id'],
                    'text': comment['textDisplay'],
                    'author': comment['authorDisplayName'],
                    'author_channel_id': comment.get('authorChannelId', {}).get('value', ''),
                    'like_count': comment['likeCount'],
                    'published_at': comment['publishedAt'],
                    'updated_at': comment['updatedAt'],
                }
                
                # ëŒ€ëŒ“ê¸€ ì²˜ë¦¬
                if 'replies' in item:
                    replies = []
                    for reply in item['replies']['comments']:
                        reply_snippet = reply['snippet']
                        replies.append({
                            'reply_id': reply['id'],
                            'text': reply_snippet['textDisplay'],
                            'author': reply_snippet['authorDisplayName'],
                            'like_count': reply_snippet['likeCount'],
                            'published_at': reply_snippet['publishedAt'],
                        })
                    comment_data['replies'] = replies
                
                comments.append(comment_data)
            
            return comments
        except Exception as e:
            print(f"Error getting comments: {e}")
            return []
    
    def get_channel_info(self, channel_id):
        """ì±„ë„ ì •ë³´ ìˆ˜ì§‘"""
        try:
            response = self.youtube.channels().list(
                part='snippet,statistics',
                id=channel_id
            ).execute()
            
            if not response['items']:
                return None
            
            channel = response['items'][0]
            snippet = channel['snippet']
            statistics = channel['statistics']
            
            return {
                'channel_id': channel_id,
                'title': snippet.get('title', ''),
                'description': snippet.get('description', ''),
                'custom_url': snippet.get('customUrl', ''),
                'published_at': snippet.get('publishedAt', ''),
                'subscriber_count': int(statistics.get('subscriberCount', 0)),
                'video_count': int(statistics.get('videoCount', 0)),
                'view_count': int(statistics.get('viewCount', 0)),
            }
        except Exception as e:
            print(f"Error getting channel info: {e}")
            return None

# ì‚¬ìš© ì˜ˆì œ
api_key = "YOUR_API_KEY"
collector = YouTubeDataCollector(api_key)

shorts_url = "https://www.youtube.com/shorts/VIDEO_ID"
video_id = collector.extract_video_id(shorts_url)

if video_id:
    # ì˜ìƒ ì •ë³´ ìˆ˜ì§‘
    video_details = collector.get_video_details(video_id)
    
    # ëŒ“ê¸€ ìˆ˜ì§‘
    comments = collector.get_video_comments(video_id, max_results=50)
    
    # ì±„ë„ ì •ë³´ ìˆ˜ì§‘
    if video_details and video_details['channel_id']:
        channel_info = collector.get_channel_info(video_details['channel_id'])
    
    # ê²°ê³¼ ì¶œë ¥
    print(json.dumps({
        'video': video_details,
        'comments': comments,
        'channel': channel_info
    }, indent=2, ensure_ascii=False))
```

---

### **ë°©ë²• 2: yt-dlp ë¼ì´ë¸ŒëŸ¬ë¦¬**

#### **ğŸ“¦ ì„¤ì¹˜**

```bash
pip install yt-dlp
```

#### **ğŸ”§ êµ¬í˜„**

```python
import yt_dlp
import json
from datetime import datetime

class YtDlpCollector:
    def __init__(self):
        self.ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'extract_flat': False,
            'writeinfojson': False,
            'writesubtitles': False,
            'writeautomaticsub': False,
        }
    
    def extract_video_info(self, url):
        """yt-dlpë¥¼ ì‚¬ìš©í•œ ì˜ìƒ ì •ë³´ ì¶”ì¶œ"""
        try:
            with yt_dlp.YoutubeDL(self.ydl_opts) as ydl:
                info = ydl.extract_info(url, download=False)
                
                return {
                    'video_id': info.get('id', ''),
                    'title': info.get('title', ''),
                    'description': info.get('description', ''),
                    'uploader': info.get('uploader', ''),
                    'uploader_id': info.get('uploader_id', ''),
                    'upload_date': info.get('upload_date', ''),
                    'duration': info.get('duration', 0),
                    'view_count': info.get('view_count', 0),
                    'like_count': info.get('like_count', 0),
                    'comment_count': info.get('comment_count', 0),
                    'tags': info.get('tags', []),
                    'categories': info.get('categories', []),
                    'thumbnail': info.get('thumbnail', ''),
                    'webpage_url': info.get('webpage_url', ''),
                    'format_note': info.get('format_note', ''),
                    'fps': info.get('fps', 0),
                    'width': info.get('width', 0),
                    'height': info.get('height', 0),
                }
        except Exception as e:
            print(f"Error extracting with yt-dlp: {e}")
            return None

# ì‚¬ìš© ì˜ˆì œ
collector = YtDlpCollector()
video_info = collector.extract_video_info("https://www.youtube.com/shorts/VIDEO_ID")

if video_info:
    print(json.dumps(video_info, indent=2, ensure_ascii=False))
```

---

### **ë°©ë²• 3: Selenium ì›¹ ìŠ¤í¬ë˜í•‘ (ë¹„ê¶Œì¥)**

#### **âš ï¸ ì£¼ì˜ì‚¬í•­**
- ë¸Œë¼ìš°ì € ìì› ì†Œëª¨ê°€ í¬ê³  ì†ë„ê°€ ëŠë¦¼
- YouTubeì˜ ë´‡ íƒì§€ì— ê±¸ë¦´ ìœ„í—˜
- UI ë³€ê²½ ì‹œ ì½”ë“œ ìˆ˜ì • í•„ìš”
- ë²•ì  ë¦¬ìŠ¤í¬ ì¡´ì¬

#### **ğŸ”§ êµ¬í˜„ (ì°¸ê³ ìš©)**

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import time
import json

class SeleniumCollector:
    def __init__(self, headless=True):
        self.options = Options()
        if headless:
            self.options.add_argument('--headless')
        self.options.add_argument('--no-sandbox')
        self.options.add_argument('--disable-dev-shm-usage')
        self.options.add_argument('--disable-blink-features=AutomationControlled')
        self.options.add_experimental_option("excludeSwitches", ["enable-automation"])
        self.options.add_experimental_option('useAutomationExtension', False)
    
    def scrape_shorts_data(self, url):
        """Seleniumì„ ì‚¬ìš©í•œ Shorts ë°ì´í„° ìŠ¤í¬ë˜í•‘"""
        driver = webdriver.Chrome(options=self.options)
        
        try:
            # ì‚¬ëŒì²˜ëŸ¼ í–‰ë™í•˜ê¸° ìœ„í•œ ì„¤ì •
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            driver.get(url)
            wait = WebDriverWait(driver, 15)
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            time.sleep(3)
            
            # ì œëª© ì¶”ì¶œ
            try:
                title_element = wait.until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "h1.ytd-watch-metadata"))
                )
                title = title_element.text
            except:
                title = "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"
            
            # ì¡°íšŒìˆ˜, ì¢‹ì•„ìš” ë“± í†µê³„ ì •ë³´ ì¶”ì¶œ
            try:
                stats_selector = "#info-container #info span"
                stats_elements = driver.find_elements(By.CSS_SELECTOR, stats_selector)
                stats_text = [elem.text for elem in stats_elements if elem.text]
            except:
                stats_text = []
            
            # ëŒ“ê¸€ ë¡œë”©ì„ ìœ„í•œ ìŠ¤í¬ë¡¤
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            # ëŒ“ê¸€ ì¶”ì¶œ
            try:
                comment_elements = driver.find_elements(By.CSS_SELECTOR, "#content-text")
                comments = []
                for elem in comment_elements[:20]:  # ìƒìœ„ 20ê°œë§Œ
                    if elem.text.strip():
                        comments.append(elem.text.strip())
            except:
                comments = []
            
            return {
                'title': title,
                'stats': stats_text,
                'comments': comments,
                'url': url,
                'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
        except Exception as e:
            print(f"ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
            return None
        
        finally:
            driver.quit()

# ì‚¬ìš© ì˜ˆì œ (ê¶Œì¥í•˜ì§€ ì•ŠìŒ)
# collector = SeleniumCollector(headless=True)
# data = collector.scrape_shorts_data("https://www.youtube.com/shorts/VIDEO_ID")
```

---

## ğŸ¯ **í†µí•© ì†”ë£¨ì…˜ (ê¶Œì¥)**

### **ìµœì í™”ëœ ë°ì´í„° ìˆ˜ì§‘ê¸°**

```python
import requests
import yt_dlp
from googleapiclient.discovery import build
import re
import time
import json
from datetime import datetime
from typing import Dict, List, Optional

class YouTubeShortsAnalyzer:
    """YouTube Shorts ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self, api_key: Optional[str] = None, rate_limit: float = 1.0):
        self.api_key = api_key
        self.rate_limit = rate_limit  # ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„(ì´ˆ)
        self.last_request_time = 0
        
        if api_key:
            self.youtube = build('youtube', 'v3', developerKey=api_key)
        
        self.ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'extract_flat': False,
        }
    
    def _rate_limit_check(self):
        """API ìš”ì²­ ì†ë„ ì œí•œ"""
        current_time = time.time()
        time_diff = current_time - self.last_request_time
        
        if time_diff < self.rate_limit:
            time.sleep(self.rate_limit - time_diff)
        
        self.last_request_time = time.time()
    
    def extract_video_id(self, url: str) -> Optional[str]:
        """YouTube URLì—ì„œ video ID ì¶”ì¶œ"""
        patterns = [
            r'(?:youtube\.com/shorts/)([^&\n?#]+)',
            r'(?:youtube\.com/watch\?v=)([^&\n?#]+)',
            r'(?:youtu\.be/)([^&\n?#]+)',
            r'(?:youtube\.com/embed/)([^&\n?#]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        return None
    
    def collect_with_yt_dlp(self, url: str) -> Optional[Dict]:
        """yt-dlpë¥¼ ì‚¬ìš©í•œ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘"""
        self._rate_limit_check()
        
        try:
            with yt_dlp.YoutubeDL(self.ydl_opts) as ydl:
                info = ydl.extract_info(url, download=False)
                
                return {
                    'basic_info': {
                        'video_id': info.get('id', ''),
                        'title': info.get('title', ''),
                        'description': info.get('description', ''),
                        'uploader': info.get('uploader', ''),
                        'uploader_id': info.get('uploader_id', ''),
                        'upload_date': info.get('upload_date', ''),
                        'duration': info.get('duration', 0),
                        'view_count': info.get('view_count', 0),
                        'like_count': info.get('like_count', 0),
                        'comment_count': info.get('comment_count', 0),
                        'tags': info.get('tags', []),
                        'categories': info.get('categories', []),
                        'thumbnail': info.get('thumbnail', ''),
                    },
                    'technical_info': {
                        'format_note': info.get('format_note', ''),
                        'fps': info.get('fps', 0),
                        'width': info.get('width', 0),
                        'height': info.get('height', 0),
                        'filesize': info.get('filesize', 0),
                    }
                }
        except Exception as e:
            print(f"yt-dlp ì˜¤ë¥˜: {e}")
            return None
    
    def collect_with_api(self, video_id: str) -> Optional[Dict]:
        """YouTube Data APIë¥¼ ì‚¬ìš©í•œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘"""
        if not self.api_key:
            return None
        
        self._rate_limit_check()
        
        try:
            # ì˜ìƒ ì •ë³´
            video_response = self.youtube.videos().list(
                part='snippet,statistics,contentDetails',
                id=video_id
            ).execute()
            
            if not video_response['items']:
                return None
            
            video = video_response['items'][0]
            snippet = video['snippet']
            statistics = video['statistics']
            
            # ëŒ“ê¸€ ìˆ˜ì§‘
            comments = self._collect_comments(video_id)
            
            # ì±„ë„ ì •ë³´
            channel_info = self._collect_channel_info(snippet['channelId'])
            
            return {
                'video_details': {
                    'video_id': video_id,
                    'title': snippet.get('title', ''),
                    'description': snippet.get('description', ''),
                    'channel_title': snippet.get('channelTitle', ''),
                    'channel_id': snippet.get('channelId', ''),
                    'published_at': snippet.get('publishedAt', ''),
                    'tags': snippet.get('tags', []),
                    'category_id': snippet.get('categoryId', ''),
                    'view_count': int(statistics.get('viewCount', 0)),
                    'like_count': int(statistics.get('likeCount', 0)),
                    'comment_count': int(statistics.get('commentCount', 0)),
                    'duration': video['contentDetails'].get('duration', ''),
                },
                'comments': comments,
                'channel_info': channel_info
            }
        except Exception as e:
            print(f"API ì˜¤ë¥˜: {e}")
            return None
    
    def _collect_comments(self, video_id: str, max_results: int = 50) -> List[Dict]:
        """ëŒ“ê¸€ ìˆ˜ì§‘"""
        comments = []
        try:
            response = self.youtube.commentThreads().list(
                part='snippet,replies',
                videoId=video_id,
                maxResults=min(max_results, 100),
                order='relevance'
            ).execute()
            
            for item in response['items']:
                comment = item['snippet']['topLevelComment']['snippet']
                comment_data = {
                    'comment_id': item['id'],
                    'text': comment['textDisplay'],
                    'author': comment['authorDisplayName'],
                    'like_count': comment['likeCount'],
                    'published_at': comment['publishedAt'],
                    'reply_count': item['snippet']['totalReplyCount']
                }
                
                # ëŒ€ëŒ“ê¸€ ì²˜ë¦¬ (ìˆëŠ” ê²½ìš°)
                if 'replies' in item and item['snippet']['totalReplyCount'] > 0:
                    replies = []
                    for reply in item['replies']['comments'][:5]:  # ìƒìœ„ 5ê°œ ëŒ€ëŒ“ê¸€ë§Œ
                        reply_snippet = reply['snippet']
                        replies.append({
                            'text': reply_snippet['textDisplay'],
                            'author': reply_snippet['authorDisplayName'],
                            'like_count': reply_snippet['likeCount'],
                            'published_at': reply_snippet['publishedAt'],
                        })
                    comment_data['replies'] = replies
                
                comments.append(comment_data)
            
        except Exception as e:
            print(f"ëŒ“ê¸€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        return comments
    
    def _collect_channel_info(self, channel_id: str) -> Optional[Dict]:
        """ì±„ë„ ì •ë³´ ìˆ˜ì§‘"""
        try:
            response = self.youtube.channels().list(
                part='snippet,statistics',
                id=channel_id
            ).execute()
            
            if not response['items']:
                return None
            
            channel = response['items'][0]
            snippet = channel['snippet']
            statistics = channel['statistics']
            
            return {
                'channel_id': channel_id,
                'title': snippet.get('title', ''),
                'description': snippet.get('description', ''),
                'published_at': snippet.get('publishedAt', ''),
                'subscriber_count': int(statistics.get('subscriberCount', 0)),
                'video_count': int(statistics.get('videoCount', 0)),
                'view_count': int(statistics.get('viewCount', 0)),
            }
        except Exception as e:
            print(f"ì±„ë„ ì •ë³´ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return None
    
    def comprehensive_analysis(self, url: str) -> Dict:
        """ì¢…í•©ì ì¸ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„"""
        video_id = self.extract_video_id(url)
        if not video_id:
            return {"error": "ì˜¬ë°”ë¥´ì§€ ì•Šì€ YouTube URLì…ë‹ˆë‹¤."}
        
        result = {
            'url': url,
            'video_id': video_id,
            'collected_at': datetime.now().isoformat(),
            'data_sources': []
        }
        
        # yt-dlpë¡œ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        yt_dlp_data = self.collect_with_yt_dlp(url)
        if yt_dlp_data:
            result.update(yt_dlp_data)
            result['data_sources'].append('yt-dlp')
        
        # APIë¡œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)
        if self.api_key:
            api_data = self.collect_with_api(video_id)
            if api_data:
                result.update(api_data)
                result['data_sources'].append('youtube_api')
        
        # ë°ì´í„° í’ˆì§ˆ í‰ê°€
        result['data_quality'] = self._assess_data_quality(result)
        
        return result
    
    def _assess_data_quality(self, data: Dict) -> Dict:
        """ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ í’ˆì§ˆ í‰ê°€"""
        quality_score = 0
        max_score = 10
        
        # ê¸°ë³¸ ì •ë³´ ì¡´ì¬ ì—¬ë¶€ (4ì )
        if data.get('basic_info', {}).get('title'):
            quality_score += 1
        if data.get('basic_info', {}).get('view_count', 0) > 0:
            quality_score += 1
        if data.get('basic_info', {}).get('description'):
            quality_score += 1
        if data.get('basic_info', {}).get('uploader'):
            quality_score += 1
        
        # API ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ (3ì )
        if data.get('video_details'):
            quality_score += 2
        if data.get('comments'):
            quality_score += 1
        
        # ì¶”ê°€ ë°ì´í„° (3ì )
        if data.get('channel_info'):
            quality_score += 1
        if data.get('technical_info'):
            quality_score += 1
        if len(data.get('data_sources', [])) > 1:
            quality_score += 1
        
        return {
            'score': quality_score,
            'max_score': max_score,
            'percentage': round((quality_score / max_score) * 100, 1),
            'grade': self._get_quality_grade(quality_score, max_score)
        }
    
    def _get_quality_grade(self, score: int, max_score: int) -> str:
        """í’ˆì§ˆ ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
        percentage = (score / max_score) * 100
        if percentage >= 90:
            return 'A+'
        elif percentage >= 80:
            return 'A'
        elif percentage >= 70:
            return 'B+'
        elif percentage >= 60:
            return 'B'
        else:
            return 'C'

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    # API í‚¤ê°€ ìˆëŠ” ê²½ìš°
    analyzer = YouTubeShortsAnalyzer(api_key="YOUR_API_KEY", rate_limit=1.0)
    
    # API í‚¤ê°€ ì—†ëŠ” ê²½ìš°
    # analyzer = YouTubeShortsAnalyzer(rate_limit=1.0)
    
    shorts_url = "https://www.youtube.com/shorts/YOUR_VIDEO_ID"
    result = analyzer.comprehensive_analysis(shorts_url)
    
    # ê²°ê³¼ ì¶œë ¥
    print(json.dumps(result, indent=2, ensure_ascii=False))
    
    # í’ˆì§ˆ í‰ê°€ ì¶œë ¥
    quality = result.get('data_quality', {})
    print(f"\në°ì´í„° í’ˆì§ˆ: {quality.get('grade')} ({quality.get('percentage')}%)")
```

---

## ğŸ’¾ **ì‹¤ì œ êµ¬í˜„ ì˜ˆì œ**

### **ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ**

```python
import csv
import time
from typing import List
import logging

class BatchShortsProcessor:
    """ì—¬ëŸ¬ Shorts URLì„ ì¼ê´„ ì²˜ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ"""
    
    def __init__(self, api_key: str = None, output_format: str = 'json'):
        self.analyzer = YouTubeShortsAnalyzer(api_key=api_key, rate_limit=2.0)
        self.output_format = output_format
        self.logger = self._setup_logger()
    
    def _setup_logger(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('shorts_collection.log'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)
    
    def process_url_list(self, urls: List[str], output_file: str = None) -> List[Dict]:
        """URL ë¦¬ìŠ¤íŠ¸ ì¼ê´„ ì²˜ë¦¬"""
        results = []
        total_urls = len(urls)
        
        self.logger.info(f"Processing {total_urls} URLs...")
        
        for i, url in enumerate(urls, 1):
            self.logger.info(f"Processing {i}/{total_urls}: {url}")
            
            try:
                result = self.analyzer.comprehensive_analysis(url)
                results.append(result)
                
                # ì„±ê³µë¥  ê³„ì‚°
                successful = len([r for r in results if 'error' not in r])
                success_rate = (successful / i) * 100
                
                self.logger.info(f"Success rate: {success_rate:.1f}% ({successful}/{i})")
                
            except Exception as e:
                self.logger.error(f"Error processing {url}: {e}")
                results.append({
                    'url': url,
                    'error': str(e),
                    'collected_at': datetime.now().isoformat()
                })
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if i % 10 == 0 or i == total_urls:
                self.logger.info(f"Progress: {i}/{total_urls} ({(i/total_urls)*100:.1f}%)")
        
        # ê²°ê³¼ ì €ì¥
        if output_file:
            self._save_results(results, output_file)
        
        return results
    
    def _save_results(self, results: List[Dict], filename: str):
        """ê²°ê³¼ ì €ì¥"""
        if self.output_format == 'json':
            with open(f"{filename}.json", 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
        
        elif self.output_format == 'csv':
            self._save_as_csv(results, f"{filename}.csv")
        
        self.logger.info(f"Results saved to {filename}.{self.output_format}")
    
    def _save_as_csv(self, results: List[Dict], filename: str):
        """CSV í˜•íƒœë¡œ ì €ì¥"""
        if not results:
            return
        
        # CSV í—¤ë” ì •ì˜
        headers = [
            'url', 'video_id', 'title', 'uploader', 'upload_date',
            'view_count', 'like_count', 'comment_count', 'duration',
            'subscriber_count', 'data_quality_score', 'collected_at'
        ]
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=headers)
            writer.writeheader()
            
            for result in results:
                if 'error' in result:
                    continue
                
                row = {
                    'url': result.get('url', ''),
                    'video_id': result.get('video_id', ''),
                    'title': result.get('basic_info', {}).get('title', ''),
                    'uploader': result.get('basic_info', {}).get('uploader', ''),
                    'upload_date': result.get('basic_info', {}).get('upload_date', ''),
                    'view_count': result.get('basic_info', {}).get('view_count', 0),
                    'like_count': result.get('basic_info', {}).get('like_count', 0),
                    'comment_count': result.get('basic_info', {}).get('comment_count', 0),
                    'duration': result.get('basic_info', {}).get('duration', 0),
                    'subscriber_count': result.get('channel_info', {}).get('subscriber_count', 0),
                    'data_quality_score': result.get('data_quality', {}).get('score', 0),
                    'collected_at': result.get('collected_at', ''),
                }
                writer.writerow(row)

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    # URL ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
    shorts_urls = [
        "https://www.youtube.com/shorts/VIDEO_ID_1",
        "https://www.youtube.com/shorts/VIDEO_ID_2",
        "https://www.youtube.com/shorts/VIDEO_ID_3",
        # ... ë” ë§ì€ URL
    ]
    
    # ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    processor = BatchShortsProcessor(
        api_key="YOUR_API_KEY",  # ì„ íƒì‚¬í•­
        output_format='csv'  # 'json' ë˜ëŠ” 'csv'
    )
    
    # ì¼ê´„ ì²˜ë¦¬ ì‹¤í–‰
    results = processor.process_url_list(
        urls=shorts_urls,
        output_file='shorts_analysis_results'
    )
    
    print(f"ì´ {len(results)}ê°œ URL ì²˜ë¦¬ ì™„ë£Œ")
```

---

## ğŸ“Š **ë°ì´í„° í™œìš© ì „ëµ**

### **ìˆ˜ì§‘ëœ ë°ì´í„° ë¶„ì„ ì˜ˆì œ**

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import numpy as np

class ShortsDataAnalyzer:
    """ìˆ˜ì§‘ëœ Shorts ë°ì´í„° ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, data_file: str):
        if data_file.endswith('.json'):
            self.df = self._load_from_json(data_file)
        elif data_file.endswith('.csv'):
            self.df = pd.read_csv(data_file)
        else:
            raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
    
    def _load_from_json(self, json_file: str) -> pd.DataFrame:
        """JSON ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        rows = []
        for item in data:
            if 'error' in item:
                continue
            
            row = {
                'video_id': item.get('video_id', ''),
                'title': item.get('basic_info', {}).get('title', ''),
                'uploader': item.get('basic_info', {}).get('uploader', ''),
                'upload_date': item.get('basic_info', {}).get('upload_date', ''),
                'view_count': item.get('basic_info', {}).get('view_count', 0),
                'like_count': item.get('basic_info', {}).get('like_count', 0),
                'comment_count': item.get('basic_info', {}).get('comment_count', 0),
                'duration': item.get('basic_info', {}).get('duration', 0),
                'subscriber_count': item.get('channel_info', {}).get('subscriber_count', 0),
            }
            rows.append(row)
        
        return pd.DataFrame(rows)
    
    def generate_performance_report(self) -> Dict:
        """ì„±ê³¼ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = {}
        
        # ê¸°ë³¸ í†µê³„
        report['basic_stats'] = {
            'total_videos': len(self.df),
            'avg_views': self.df['view_count'].mean(),
            'avg_likes': self.df['like_count'].mean(),
            'avg_comments': self.df['comment_count'].mean(),
            'total_views': self.df['view_count'].sum(),
        }
        
        # ìƒìœ„ ì„±ê³¼ ì˜ìƒ
        report['top_performers'] = {
            'most_viewed': self.df.nlargest(5, 'view_count')[['title', 'view_count']].to_dict('records'),
            'most_liked': self.df.nlargest(5, 'like_count')[['title', 'like_count']].to_dict('records'),
            'most_commented': self.df.nlargest(5, 'comment_count')[['title', 'comment_count']].to_dict('records'),
        }
        
        # ì°¸ì—¬ìœ¨ ë¶„ì„
        self.df['engagement_rate'] = (self.df['like_count'] + self.df['comment_count']) / self.df['view_count'] * 100
        report['engagement'] = {
            'avg_engagement_rate': self.df['engagement_rate'].mean(),
            'high_engagement_threshold': self.df['engagement_rate'].quantile(0.8),
            'top_engagement_videos': self.df.nlargest(5, 'engagement_rate')[['title', 'engagement_rate']].to_dict('records'),
        }
        
        # ì±„ë„ë³„ ë¶„ì„
        channel_stats = self.df.groupby('uploader').agg({
            'view_count': ['count', 'mean', 'sum'],
            'like_count': 'mean',
            'engagement_rate': 'mean'
        }).round(2)
        
        report['channel_analysis'] = channel_stats.to_dict()
        
        return report
    
    def create_visualizations(self, output_dir: str = './charts'):
        """ë°ì´í„° ì‹œê°í™” ìƒì„±"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        plt.style.use('seaborn-v0_8')
        
        # 1. ì¡°íšŒìˆ˜ ë¶„í¬
        plt.figure(figsize=(12, 6))
        plt.hist(self.df['view_count'], bins=30, alpha=0.7, color='skyblue')
        plt.xlabel('ì¡°íšŒìˆ˜')
        plt.ylabel('ì˜ìƒ ìˆ˜')
        plt.title('ì¡°íšŒìˆ˜ ë¶„í¬')
        plt.ticklabel_format(style='plain', axis='x')
        plt.savefig(f'{output_dir}/view_count_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. ì¡°íšŒìˆ˜ vs ì¢‹ì•„ìš” ìˆ˜ ìƒê´€ê´€ê³„
        plt.figure(figsize=(10, 8))
        plt.scatter(self.df['view_count'], self.df['like_count'], alpha=0.6)
        plt.xlabel('ì¡°íšŒìˆ˜')
        plt.ylabel('ì¢‹ì•„ìš” ìˆ˜')
        plt.title('ì¡°íšŒìˆ˜ vs ì¢‹ì•„ìš” ìˆ˜ ìƒê´€ê´€ê³„')
        
        # ì¶”ì„¸ì„  ì¶”ê°€
        z = np.polyfit(self.df['view_count'], self.df['like_count'], 1)
        p = np.poly1d(z)
        plt.plot(self.df['view_count'], p(self.df['view_count']), "r--", alpha=0.8)
        
        plt.savefig(f'{output_dir}/views_vs_likes.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. ìƒìœ„ ì±„ë„ë³„ ì„±ê³¼
        top_channels = self.df.groupby('uploader')['view_count'].sum().nlargest(10)
        
        plt.figure(figsize=(12, 8))
        top_channels.plot(kind='barh')
        plt.xlabel('ì´ ì¡°íšŒìˆ˜')
        plt.title('ìƒìœ„ 10ê°œ ì±„ë„ë³„ ì´ ì¡°íšŒìˆ˜')
        plt.tight_layout()
        plt.savefig(f'{output_dir}/top_channels.png', dpi=300, bbox_inches='tight')
        plt.close()

# ì‚¬ìš© ì˜ˆì œ
analyzer = ShortsDataAnalyzer('shorts_analysis_results.csv')
report = analyzer.generate_performance_report()
analyzer.create_visualizations()

print("ë¶„ì„ ë¦¬í¬íŠ¸:")
print(json.dumps(report, indent=2, ensure_ascii=False))
```

---

## ğŸ¯ **í”„ë¡œì íŠ¸ í…œí”Œë¦¿**

### **ì™„ì „í•œ í”„ë¡œì íŠ¸ êµ¬ì¡°**

```
youtube_shorts_analyzer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ collectors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ api_collector.py
â”‚   â”‚   â”œâ”€â”€ ytdlp_collector.py
â”‚   â”‚   â””â”€â”€ unified_collector.py
â”‚   â”œâ”€â”€ analyzers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_analyzer.py
â”‚   â”‚   â””â”€â”€ visualization.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ url_parser.py
â”‚   â”‚   â”œâ”€â”€ rate_limiter.py
â”‚   â”‚   â””â”€â”€ data_validator.py
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ settings.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ results/
â”œâ”€â”€ logs/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_collectors.py
â”‚   â”œâ”€â”€ test_analyzers.py
â”‚   â””â”€â”€ test_utils.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ main.py
â””â”€â”€ config.yaml
```

### **requirements.txt**

```txt
google-api-python-client==2.108.0
yt-dlp==2023.10.13
pandas==2.1.3
matplotlib==3.8.1
seaborn==0.13.0
requests==2.31.0
pyyaml==6.0.1
python-dotenv==1.0.0
tqdm==4.66.1
selenium==4.15.2
beautifulsoup4==4.12.2
lxml==4.9.3
numpy==1.25.2
plotly==5.17.0
```

### **config.yaml**

```yaml
# YouTube API ì„¤ì •
youtube_api:
  key: ${YOUTUBE_API_KEY}
  quota_limit: 10000
  requests_per_second: 10

# ìˆ˜ì§‘ ì„¤ì •
collection:
  rate_limit: 1.0  # ì´ˆ
  max_comments: 100
  max_retries: 3
  timeout: 30

# ë°ì´í„° ì„¤ì •
data:
  output_format: "json"  # json, csv, both
  output_directory: "./data/results"
  log_directory: "./logs"

# ë¶„ì„ ì„¤ì •
analysis:
  create_visualizations: true
  chart_directory: "./data/charts"
  report_format: "markdown"  # json, markdown, html
```

### **main.py**

```python
#!/usr/bin/env python3
"""
YouTube Shorts ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
"""

import argparse
import yaml
import os
from dotenv import load_dotenv
from src.collectors.unified_collector import YouTubeShortsAnalyzer
from src.analyzers.data_analyzer import ShortsDataAnalyzer

def load_config(config_path: str = "config.yaml") -> dict:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    load_dotenv()  # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # í™˜ê²½ ë³€ìˆ˜ë¡œ API í‚¤ ì„¤ì •
    if 'YOUTUBE_API_KEY' in os.environ:
        config['youtube_api']['key'] = os.environ['YOUTUBE_API_KEY']
    
    return config

def main():
    parser = argparse.ArgumentParser(description='YouTube Shorts ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„')
    parser.add_argument('--urls', nargs='+', help='ë¶„ì„í•  YouTube Shorts URL ëª©ë¡')
    parser.add_argument('--url-file', help='URLì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ íŒŒì¼')
    parser.add_argument('--output', default='shorts_analysis', help='ì¶œë ¥ íŒŒì¼ëª…')
    parser.add_argument('--config', default='config.yaml', help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--analyze-only', help='ê¸°ì¡´ ë°ì´í„° íŒŒì¼ ë¶„ì„ë§Œ ìˆ˜í–‰')
    
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ
    config = load_config(args.config)
    
    if args.analyze_only:
        # ê¸°ì¡´ ë°ì´í„° ë¶„ì„ë§Œ ìˆ˜í–‰
        analyzer = ShortsDataAnalyzer(args.analyze_only)
        report = analyzer.generate_performance_report()
        
        if config['analysis']['create_visualizations']:
            analyzer.create_visualizations(config['analysis']['chart_directory'])
        
        print("ë¶„ì„ ì™„ë£Œ!")
        return
    
    # URL ì¤€ë¹„
    urls = []
    if args.urls:
        urls.extend(args.urls)
    
    if args.url_file:
        with open(args.url_file, 'r', encoding='utf-8') as f:
            urls.extend([line.strip() for line in f if line.strip()])
    
    if not urls:
        print("ë¶„ì„í•  URLì„ ì œê³µí•´ì£¼ì„¸ìš”.")
        return
    
    # ë°ì´í„° ìˆ˜ì§‘
    analyzer = YouTubeShortsAnalyzer(
        api_key=config['youtube_api'].get('key'),
        rate_limit=config['collection']['rate_limit']
    )
    
    print(f"{len(urls)}ê°œ URL ìˆ˜ì§‘ ì‹œì‘...")
    
    results = []
    for i, url in enumerate(urls, 1):
        print(f"Processing {i}/{len(urls)}: {url}")
        result = analyzer.comprehensive_analysis(url)
        results.append(result)
    
    # ê²°ê³¼ ì €ì¥
    output_dir = config['data']['output_directory']
    os.makedirs(output_dir, exist_ok=True)
    
    if config['data']['output_format'] in ['json', 'both']:
        with open(f"{output_dir}/{args.output}.json", 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
    
    if config['data']['output_format'] in ['csv', 'both']:
        # CSV ì €ì¥ ë¡œì§ (ì´ì „ ì˜ˆì œ ì°¸ì¡°)
        pass
    
    print(f"ìˆ˜ì§‘ ì™„ë£Œ! ê²°ê³¼: {output_dir}/{args.output}")

if __name__ == "__main__":
    main()
```

---

## ğŸ“ˆ **ì„±ëŠ¥ ìµœì í™” íŒ**

### **1. API í• ë‹¹ëŸ‰ ê´€ë¦¬**

```python
class QuotaManager:
    """YouTube API í• ë‹¹ëŸ‰ ê´€ë¦¬"""
    
    def __init__(self, daily_quota: int = 10000):
        self.daily_quota = daily_quota
        self.used_quota = 0
        self.operations_cost = {
            'videos.list': 1,
            'commentThreads.list': 1,
            'channels.list': 1,
            'search.list': 100,  # ë¹„ìš©ì´ ë†’ìŒ
        }
    
    def check_quota(self, operation: str, count: int = 1) -> bool:
        """í• ë‹¹ëŸ‰ í™•ì¸"""
        cost = self.operations_cost.get(operation, 1) * count
        return (self.used_quota + cost) <= self.daily_quota
    
    def use_quota(self, operation: str, count: int = 1):
        """í• ë‹¹ëŸ‰ ì‚¬ìš©"""
        cost = self.operations_cost.get(operation, 1) * count
        self.used_quota += cost
        
        remaining = self.daily_quota - self.used_quota
        print(f"Quota used: {cost}, Remaining: {remaining}")
```

### **2. ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„**

```python
import time
from functools import wraps

def retry_on_error(max_retries: int = 3, delay: float = 1.0):
    """ì—ëŸ¬ ì‹œ ì¬ì‹œë„ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise e
                    
                    wait_time = delay * (2 ** attempt)  # ì§€ìˆ˜ì  ë°±ì˜¤í”„
                    print(f"Attempt {attempt + 1} failed: {e}")
                    print(f"Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
            
            return None
        return wrapper
    return decorator
```

---

## ğŸ”§ **ë¬¸ì œ í•´ê²° ê°€ì´ë“œ**

### **ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤**

#### **1. API í• ë‹¹ëŸ‰ ì´ˆê³¼**
```
Error: quotaExceeded
```
**í•´ê²° ë°©ë²•:**
- í•˜ë£¨ ëŒ€ê¸° í›„ ì¬ì‹œë„
- ì—¬ëŸ¬ API í‚¤ ë¡œí…Œì´ì…˜ ì‚¬ìš©
- yt-dlpë§Œ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘

#### **2. ë¹„ê³µê°œ/ì‚­ì œëœ ì˜ìƒ**
```
Error: Video unavailable
```
**í•´ê²° ë°©ë²•:**
- URL ìœ íš¨ì„± ì‚¬ì „ ê²€ì¦
- ì—ëŸ¬ ì²˜ë¦¬ë¡œ ìŠ¤í‚µí•˜ê³  ê³„ì† ì§„í–‰

#### **3. ì§€ì—­ ì œí•œ ì˜ìƒ**
```
Error: Video not available in your country
```
**í•´ê²° ë°©ë²•:**
- VPN ì‚¬ìš© (ì£¼ì˜: ì´ìš©ì•½ê´€ í™•ì¸)
- í•´ë‹¹ ì˜ìƒ ìŠ¤í‚µí•˜ê³  ì§„í–‰

### **ë””ë²„ê¹… ë„êµ¬**

```python
import logging
from datetime import datetime

def setup_detailed_logging():
    """ìƒì„¸í•œ ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(f'debug_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
            logging.StreamHandler()
        ]
    )

def validate_url(url: str) -> bool:
    """URL ìœ íš¨ì„± ê²€ì¦"""
    patterns = [
        r'https?://(?:www\.)?youtube\.com/shorts/[A-Za-z0-9_-]+',
        r'https?://(?:www\.)?youtube\.com/watch\?v=[A-Za-z0-9_-]+',
        r'https?://youtu\.be/[A-Za-z0-9_-]+'
    ]
    
    return any(re.match(pattern, url) for pattern in patterns)
```

---

## ğŸ“ **ë§ˆë¬´ë¦¬ ë° ê¶Œì¥ì‚¬í•­**

### **ğŸ¯ ìµœì¢… ê¶Œì¥ ì‚¬í•­**

1. **ê³µì‹ API ìš°ì„  ì‚¬ìš©**: YouTube Data API v3ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
2. **ì†ë„ ì œí•œ ì¤€ìˆ˜**: ìš”ì²­ ê°„ ì ì ˆí•œ ê°„ê²© ìœ ì§€ (1-2ì´ˆ)
3. **ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”**: ë‹¤ì–‘í•œ ì˜ˆì™¸ ìƒí™©ì— ëŒ€í•œ ëŒ€ë¹„ì±… ë§ˆë ¨
4. **ë°ì´í„° ë°±ì—…**: ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ ì •ê¸°ì ì¸ ë°±ì—… ë° ë²„ì „ ê´€ë¦¬
5. **ë²•ì  ì¤€ìˆ˜**: YouTube ì´ìš©ì•½ê´€ ë° ê´€ë ¨ ë²•ê·œ ì¤€ìˆ˜

### **ğŸš€ ë‹¤ìŒ ë‹¨ê³„ ë°œì „ ë°©í–¥**

1. **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: íŠ¹ì • ì±„ë„ì´ë‚˜ í‚¤ì›Œë“œì˜ ì‹¤ì‹œê°„ Shorts ìˆ˜ì§‘
2. **ìë™ ë¶„ì„**: AIë¥¼ í™œìš©í•œ ì½˜í…ì¸  íŠ¸ë Œë“œ ìë™ ë¶„ì„
3. **ëŒ€ì‹œë³´ë“œ êµ¬ì¶•**: ì›¹ ê¸°ë°˜ ì‹¤ì‹œê°„ ë¶„ì„ ëŒ€ì‹œë³´ë“œ ê°œë°œ
4. **ì˜ˆì¸¡ ëª¨ë¸**: ì¡°íšŒìˆ˜/ì„±ê³¼ ì˜ˆì¸¡ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ê°œë°œ

### **ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ**

- [YouTube Data API v3 ê³µì‹ ë¬¸ì„œ](https://developers.google.com/youtube/v3)
- [yt-dlp GitHub ì €ì¥ì†Œ](https://github.com/yt-dlp/yt-dlp)
- [YouTube ê°œë°œì ì •ì±…](https://developers.google.com/youtube/terms/developer-policies)
- [Google API í• ë‹¹ëŸ‰ ê´€ë¦¬](https://developers.google.com/youtube/v3/getting-started#quota)

---

*ì´ ê°€ì´ë“œëŠ” YouTube Shorts ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ í¬ê´„ì ì¸ ë°©ë²•ë¡ ì„ ì œì‹œí•˜ë©°, ë²•ì  ì¤€ìˆ˜ì™€ ê¸°ìˆ ì  ìµœì í™”ë¥¼ ëª¨ë‘ ê³ ë ¤í•œ ì‹¤ìš©ì ì¸ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.*

**âš ï¸ ì¤‘ìš” ë©´ì±… ì¡°í•­**: ë°ì´í„° ìˆ˜ì§‘ ì „ ë°˜ë“œì‹œ YouTube ì´ìš©ì•½ê´€ì„ í™•ì¸í•˜ê³ , ìˆ˜ì§‘ëœ ë°ì´í„°ëŠ” ê°œì¸ì /êµìœ¡ì  ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

